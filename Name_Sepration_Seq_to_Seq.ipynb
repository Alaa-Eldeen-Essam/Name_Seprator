{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3d0l1awNIO3q"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import torch\n",
        "from transformers import (\n",
        "    T5Tokenizer,\n",
        "    T5ForConditionalGeneration,\n",
        "    Seq2SeqTrainingArguments,\n",
        "    Seq2SeqTrainer,\n",
        "    DataCollatorForSeq2Seq,EarlyStoppingCallback\n",
        ")\n",
        "\n",
        "from datasets import Dataset\n",
        "from sklearn.model_selection import train_test_split\n",
        "import numpy as np\n",
        "\n",
        "print(\"=\"*60)\n",
        "print(\"ARABIC NAME SEGMENTATION TRAINER - WORKING VERSION\")\n",
        "print(\"=\"*60)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4MbETjrkISa4"
      },
      "outputs": [],
      "source": [
        "\n",
        "# Or load from CSV:\n",
        "df = pd.read_csv('/content/arabic_name_segmentation_dataset_20k_modified.csv')\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HTkfEZLbfC9B"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Check for duplicates\n",
        "duplicates_count = df.duplicated().sum()\n",
        "print(f\"Number of duplicate rows: {duplicates_count}\")\n",
        "\n",
        "# Store original length\n",
        "original_length = len(df)\n",
        "\n",
        "# Drop duplicates\n",
        "df = df.drop_duplicates()\n",
        "\n",
        "# Show results\n",
        "new_length = len(df)\n",
        "rows_removed = original_length - new_length\n",
        "\n",
        "print(f\"\\nOriginal rows: {original_length}\")\n",
        "print(f\"Rows after removing duplicates: {new_length}\")\n",
        "print(f\"Rows removed: {rows_removed}\")\n",
        "\n",
        "# Reset index after dropping duplicates\n",
        "df = df.reset_index(drop=True)\n",
        "print(\"\\n✓ Duplicates removed and index reset\")\n",
        "\n",
        "# Display first few rows\n",
        "print(\"\\nFirst few rows after cleaning:\")\n",
        "print(df.head())"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "data = {'input': df['input'].tolist(), 'target': df['target'].tolist()}\n",
        "print(f\"Total samples: {len(data['input'])}\")"
      ],
      "metadata": {
        "id": "OatYzHNChOeJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DZjfZosoKZbC"
      },
      "outputs": [],
      "source": [
        "if len(data['input']) > 10:\n",
        "    # Create a temporary DataFrame to easily drop rows with NaN in 'input' or 'target'\n",
        "    temp_df = pd.DataFrame(data)\n",
        "    temp_df.dropna(subset=['input', 'target'], inplace=True)\n",
        "\n",
        "    train_inputs, val_inputs, train_targets, val_targets = train_test_split(\n",
        "        temp_df['input'].tolist(), temp_df['target'].tolist(), test_size=0.1, random_state=42\n",
        "    )\n",
        "else:\n",
        "    # For small datasets, use all for training\n",
        "    print(\"WARNING: Small dataset detected. Using 80/20 split.\")\n",
        "    temp_df = pd.DataFrame(data)\n",
        "    temp_df.dropna(subset=['input', 'target'], inplace=True)\n",
        "    train_inputs, val_inputs, train_targets, val_targets = train_test_split(\n",
        "        temp_df['input'].tolist(), temp_df['target'].tolist(), test_size=0.2, random_state=42\n",
        "    )\n",
        "\n",
        "print(f\"Training samples: {len(train_inputs)}\")\n",
        "print(f\"Validation samples: {len(val_inputs)}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JQy6vpFPIlzA"
      },
      "outputs": [],
      "source": [
        "# ============= 2. LOAD MODEL (Use T5-small instead) =============\n",
        "print(\"\\nLoading T5-small model (more stable than ByT5)...\")\n",
        "model_name = \"t5-small\"\n",
        "tokenizer = T5Tokenizer.from_pretrained(model_name, legacy=False)\n",
        "model = T5ForConditionalGeneration.from_pretrained(model_name)\n",
        "\n",
        "print(f\"✓ Model loaded: {model_name}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NDZA2uaHN_Sf"
      },
      "outputs": [],
      "source": [
        "# ============= 3. CREATE DATASETS =============\n",
        "def create_dataset(inputs, targets):\n",
        "    \"\"\"Create dataset with proper formatting\"\"\"\n",
        "    formatted_inputs = [f\"segment arabic name: {text}\" for text in inputs]\n",
        "\n",
        "    # Tokenize\n",
        "    input_encodings = tokenizer(\n",
        "        formatted_inputs,\n",
        "        padding=False,\n",
        "        truncation=True,\n",
        "        max_length=128,\n",
        "        return_tensors=None\n",
        "    )\n",
        "\n",
        "    target_encodings = tokenizer(\n",
        "        targets,\n",
        "        padding=False,\n",
        "        truncation=True,\n",
        "        max_length=128,\n",
        "        return_tensors=None\n",
        "    )\n",
        "\n",
        "    # Create dataset\n",
        "    dataset_dict = {\n",
        "        'input_ids': input_encodings['input_ids'],\n",
        "        'attention_mask': input_encodings['attention_mask'],\n",
        "        'labels': target_encodings['input_ids']\n",
        "    }\n",
        "\n",
        "    return Dataset.from_dict(dataset_dict)\n",
        "\n",
        "print(\"\\nCreating datasets...\")\n",
        "train_dataset = create_dataset(train_inputs, train_targets)\n",
        "val_dataset = create_dataset(val_inputs, val_targets)\n",
        "\n",
        "print(f\"✓ Train dataset: {len(train_dataset)} samples\")\n",
        "print(f\"✓ Val dataset: {len(val_dataset)} samples\")\n",
        "\n",
        "# Verify data\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"SAMPLE DATA VERIFICATION:\")\n",
        "print(\"=\"*60)\n",
        "sample_idx = 0\n",
        "print(f\"Input text: {train_inputs[sample_idx]}\")\n",
        "print(f\"Target text: {train_targets[sample_idx]}\")\n",
        "print(f\"Input tokens: {train_dataset[sample_idx]['input_ids'][:15]}...\")\n",
        "print(f\"Label tokens: {train_dataset[sample_idx]['labels'][:10]}...\")\n",
        "print(\"=\"*60 + \"\\n\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RuwhecmDOBm-"
      },
      "outputs": [],
      "source": [
        "# ============= 4. DATA COLLATOR =============\n",
        "data_collator = DataCollatorForSeq2Seq(\n",
        "    tokenizer=tokenizer,\n",
        "    model=model,\n",
        "    label_pad_token_id=-100,\n",
        "    padding=True\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Fo-Z0MsQOBjb"
      },
      "outputs": [],
      "source": [
        "# ============= 5. TRAINING ARGUMENTS =============\n",
        "training_args = Seq2SeqTrainingArguments(\n",
        "    output_dir=\"./t5-name-segmenter\",\n",
        "    eval_strategy=\"epoch\",\n",
        "    save_strategy=\"epoch\",\n",
        "    learning_rate=3e-4,\n",
        "    per_device_train_batch_size=8,\n",
        "    per_device_eval_batch_size=8,\n",
        "    num_train_epochs=25,\n",
        "    weight_decay=0.01,\n",
        "    save_total_limit=3,\n",
        "    predict_with_generate=True,\n",
        "    load_best_model_at_end=True,\n",
        "    metric_for_best_model=\"eval_loss\",\n",
        "    logging_steps=10,\n",
        "    warmup_steps=100,\n",
        "    fp16=torch.cuda.is_available(),\n",
        "    report_to=\"none\",\n",
        "    generation_max_length=128,\n",
        "    generation_num_beams=4,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yo9LmA9eOBgH"
      },
      "outputs": [],
      "source": [
        "# ============= 6. INITIALIZE TRAINER =============\n",
        "trainer = Seq2SeqTrainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=train_dataset,\n",
        "    eval_dataset=val_dataset,\n",
        "    data_collator=data_collator,\n",
        "    tokenizer=tokenizer,\n",
        "    callbacks=[EarlyStoppingCallback(early_stopping_patience=3)]\n",
        "\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/",
          "height": 753
        },
        "id": "J5lhKqtlOW5d",
        "outputId": "8f369626-9890-4da9-8422-c2bccb035d5f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "============================================================\n",
            "STARTING TRAINING\n",
            "============================================================\n",
            "This may take 15-30 minutes depending on your hardware...\n",
            "\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='20250' max='56250' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [20250/56250 31:03 < 55:12, 10.87 it/s, Epoch 9/25]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Epoch</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>0.033800</td>\n",
              "      <td>0.020765</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2</td>\n",
              "      <td>0.017300</td>\n",
              "      <td>0.006550</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3</td>\n",
              "      <td>0.008900</td>\n",
              "      <td>0.006140</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4</td>\n",
              "      <td>0.004000</td>\n",
              "      <td>0.003694</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>5</td>\n",
              "      <td>0.004200</td>\n",
              "      <td>0.003191</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>6</td>\n",
              "      <td>0.003000</td>\n",
              "      <td>0.002667</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>7</td>\n",
              "      <td>0.006900</td>\n",
              "      <td>0.003095</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>8</td>\n",
              "      <td>0.007800</td>\n",
              "      <td>0.003007</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>9</td>\n",
              "      <td>0.002200</td>\n",
              "      <td>0.002862</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "There were missing keys in the checkpoint model loaded: ['encoder.embed_tokens.weight', 'decoder.embed_tokens.weight', 'lm_head.weight'].\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "============================================================\n",
            "TRAINING COMPLETED!\n",
            "============================================================\n",
            "Final train loss: 0.0371\n"
          ]
        }
      ],
      "source": [
        "# ============= 7. TRAIN =============\n",
        "print(\"=\"*60)\n",
        "print(\"STARTING TRAINING\")\n",
        "print(\"=\"*60)\n",
        "print(\"This may take 15-30 minutes depending on your hardware...\")\n",
        "print()\n",
        "\n",
        "train_result = trainer.train()\n",
        "\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"TRAINING COMPLETED!\")\n",
        "print(\"=\"*60)\n",
        "print(f\"Final train loss: {train_result.training_loss:.4f}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "G-l4oxD0OW2g"
      },
      "outputs": [],
      "source": [
        "# ============= 8. SAVE MODEL =============\n",
        "print(\"\\nSaving model...\")\n",
        "model.save_pretrained(\"./t5-name-segmenter-final\")\n",
        "tokenizer.save_pretrained(\"./t5-name-segmenter-final\")\n",
        "print(\"✓ Model saved to './t5-name-segmenter-final'\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "D5sPeMH1e0D1"
      },
      "outputs": [],
      "source": [
        "# Create zip file\n",
        "print(\"\\nCreating zip file...\")\n",
        "zip_filename = \"t5-name-segmenter-final\"\n",
        "shutil.make_archive(zip_filename, 'zip', './t5-name-segmenter-final')\n",
        "print(f\"✓ Model zipped to '{zip_filename}.zip'\")\n",
        "\n",
        "# Get zip file size\n",
        "zip_size = os.path.getsize(f\"{zip_filename}.zip\") / (1024 * 1024)  # Convert to MB\n",
        "print(f\"✓ Zip file size: {zip_size:.2f} MB\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qvMl_S0POWzS"
      },
      "outputs": [],
      "source": [
        "# ============= 9. EVALUATION =============\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"EVALUATING MODEL\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "eval_results = trainer.evaluate()\n",
        "print(f\"Validation Loss: {eval_results['eval_loss']:.4f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "atg6eVFtOWwP"
      },
      "outputs": [],
      "source": [
        "\n",
        "# ============= 10. INFERENCE FUNCTION =============\n",
        "def segment_name(name, model, tokenizer):\n",
        "    \"\"\"Segment a name using the trained model\"\"\"\n",
        "    model.eval()\n",
        "\n",
        "    input_text = f\"segment arabic name: {name}\"\n",
        "    inputs = tokenizer(input_text, return_tensors=\"pt\", max_length=128, truncation=True)\n",
        "\n",
        "    # Move to device\n",
        "    device = next(model.parameters()).device\n",
        "    inputs = {k: v.to(device) for k, v in inputs.items()}\n",
        "\n",
        "    with torch.no_grad():\n",
        "        outputs = model.generate(\n",
        "            **inputs,\n",
        "            max_length=128,\n",
        "            num_beams=4,\n",
        "            early_stopping=True,\n",
        "            no_repeat_ngram_size=2,\n",
        "            repetition_penalty=1.5\n",
        "        )\n",
        "\n",
        "    result = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "    return result"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Y0x2qSQsOWtC"
      },
      "outputs": [],
      "source": [
        "# ============= 11. TEST ON VALIDATION DATA =============\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"TESTING ON VALIDATION SAMPLES\")\n",
        "print(\"=\"*60 + \"\\n\")\n",
        "\n",
        "correct = 0\n",
        "total = min(len(val_inputs), 5)\n",
        "\n",
        "for i in range(total):\n",
        "    original = val_inputs[i]\n",
        "    expected = val_targets[i]\n",
        "    predicted = segment_name(original, model, tokenizer)\n",
        "\n",
        "    is_correct = predicted.strip().lower() == expected.strip().lower()\n",
        "    if is_correct:\n",
        "        correct += 1\n",
        "\n",
        "    print(f\"Input:     {original}\")\n",
        "    print(f\"Expected:  {expected}\")\n",
        "    print(f\"Predicted: {predicted}\")\n",
        "    print(f\"Status:    {'✓ CORRECT' if is_correct else '✗ INCORRECT'}\")\n",
        "    print()\n",
        "\n",
        "print(f\"Accuracy on sample: {correct}/{total} ({100*correct/total:.1f}%)\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jcC-4kzZOBYa"
      },
      "outputs": [],
      "source": [
        "# ============= 12. TEST ON NEW NAMES =============\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"TESTING ON NEW UNSEEN NAMES\")\n",
        "print(\"=\"*60 + \"\\n\")\n",
        "\n",
        "test_names = [\n",
        "    'mohamedaliahmed',\n",
        "    'hassanibrahimkhalid',\n",
        "    'fatimamohamedsaid',\n",
        "    'abdullahomarhassan',\n",
        "    'khaledyoussefali'\n",
        "]\n",
        "\n",
        "for name in test_names:\n",
        "    segmented = segment_name(name, model, tokenizer)\n",
        "    print(f\"{name:25s} → {segmented}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fhPWhknQPjpR"
      },
      "outputs": [],
      "source": [
        "# ============= 13. USAGE INSTRUCTIONS =============\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"HOW TO USE THE TRAINED MODEL\")\n",
        "print(\"=\"*60)\n",
        "print(\"\"\"\n",
        "# Load the model later:\n",
        "from transformers import T5Tokenizer, T5ForConditionalGeneration\n",
        "\n",
        "tokenizer = T5Tokenizer.from_pretrained('./t5-name-segmenter-final')\n",
        "model = T5ForConditionalGeneration.from_pretrained('./t5-name-segmenter-final')\n",
        "\n",
        "def segment_name(name):\n",
        "    inputs = tokenizer(f\"segment arabic name: {name}\", return_tensors=\"pt\")\n",
        "    outputs = model.generate(**inputs, max_length=128, num_beams=4)\n",
        "    return tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "\n",
        "# Use it:\n",
        "result = segment_name('mohamedaliahmed')\n",
        "print(result)  # Mohamed Ali Ahmed\n",
        "\"\"\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "adGUhLULPjfe"
      },
      "outputs": [],
      "source": [
        "from transformers import T5Tokenizer, T5ForConditionalGeneration\n",
        "\n",
        "tokenizer = T5Tokenizer.from_pretrained('./t5-name-segmenter-final')\n",
        "model = T5ForConditionalGeneration.from_pretrained('./t5-name-segmenter-final')\n",
        "\n",
        "def segment_name(name):\n",
        "    inputs = tokenizer(f\"segment arabic name: {name}\", return_tensors=\"pt\")\n",
        "    outputs = model.generate(**inputs, max_length=128, num_beams=4)\n",
        "    return tokenizer.decode(outputs[0], skip_special_tokens=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "T1GxBTWfPjRV"
      },
      "outputs": [],
      "source": [
        "result = segment_name('ahmedmohamedabdelsalam')\n",
        "print(result)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "y6Xbirm4PjEG"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}